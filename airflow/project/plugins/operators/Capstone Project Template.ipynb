{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "I am doing this project based on data that I have put together. I will describe the data and its sources in Step 1.\n",
    "\n",
    "#### Project Summary\n",
    "InvestSure is an investment company that manages the retirement accounts of employees of its customers. It gets a dump of many data elements in CSV format from transactional systems and has been using Excel to load this data for analysis. However, the data has now grown to a size where this approach is no longer viable. Therefore, InvestSure has hired me as a Data Engineer to analyze this data, cleanse it, build a conceptual model for analytical use of the data and load the data from Excel files into the analytical tables. InvestSure has also requested me to provide them with typical queries that they could run on this analytical model to gain insights into this data.\n",
    "\n",
    "The project follows the steps listed below:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import configparser\n",
    "from datetime import datetime, timedelta, date\n",
    "from dateutil import parser\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id, to_date, when\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek, unix_timestamp, from_unixtime\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# To suppress numeric values from being returned in exponential format\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "# suppress warnings from final output\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Read project data configuration entries\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('capstone_project_data.cfg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The scope of this project is to analyze the data provided by transactional systems, cleanse the data if needed and load it into an analytical data model to facilitate querying the data.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "In this section, I will describe the data including its source.\n",
    "\n",
    "- txn.csv (fact)\n",
    "    - Structure\n",
    "        - txn_id\n",
    "        - txn_date\n",
    "        - contact_id\n",
    "        - product_id\n",
    "        - sales\n",
    "        - redemptions\n",
    "    - Source: InvestSure's transactional system captured by its trading application\n",
    "    - Feed frequency: Daily\n",
    "\n",
    "- customer.csv (dimension)\n",
    "    - Structure\n",
    "        - customer_id\n",
    "        - customer_name\n",
    "        - sector\n",
    "    - Source: InvestSure's CRM system\n",
    "    - Feed Frequency: Daily\n",
    "\n",
    "- contact.csv (dimenstion)\n",
    "    - Structure\n",
    "        - contact_id\n",
    "        - first_name\n",
    "        - last_name\n",
    "        - city\n",
    "        - state_code\n",
    "        - zip\n",
    "        - country\n",
    "        - latitude\n",
    "        - longitude\n",
    "        - customer_id\n",
    "        - status\n",
    "        - opportunity\n",
    "    - Source: InvestSure's CRM system\n",
    "    - Feed Frequency: Daily\n",
    "\n",
    "- product.json (dimension)\n",
    "    - Structure\n",
    "        - product_id\n",
    "        - product_name\n",
    "        - tna\n",
    "        - ms_rating\n",
    "        - exp_ratio\n",
    "        - market_cap\n",
    "    - Source: Yahoo Finance\n",
    "    - Feed Frequency: Daily\n",
    "\n",
    "- sec_codes.csv (mapping table)\n",
    "    - Structure\n",
    "        - code\n",
    "        - description\n",
    "    - Source: Yahoo Finance provides description but InvestSure's systems use abbreviated codes\n",
    "    - Feed Frequency: On demand and when new customers are added\n",
    "\n",
    "- state.csv\n",
    "    - Structure\n",
    "        - state_code\n",
    "        - state\n",
    "        - region\n",
    "    - Source: US Census Board\n",
    "    - Feed Frequency: one time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "toggleable": false,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-715c3181": {
       "style": "primary"
      }
     }
    }
   },
   "source": [
    "#### Describe and Gather Data \n",
    "In the next few cells, I will gather the data and display samples for a first look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Loading the input data into Pandas dataframes\n",
    "pd_df_customer = pd.read_csv(config['LOCAL']['INPUT_DATA_CUSTOMER'], encoding = 'ISO-8859-1')\n",
    "pd_df_contact = pd.read_csv(config['LOCAL']['INPUT_DATA_CONTACT'], encoding = 'ISO-8859-1')\n",
    "pd_df_product = pd.read_json(config['LOCAL']['INPUT_DATA_PRODUCT'])\n",
    "pd_df_sec_codes = pd.read_csv(config['LOCAL']['INPUT_DATA_SECTOR'], encoding = 'ISO-8859-1')\n",
    "pd_df_state = pd.read_csv(config['LOCAL']['INPUT_DATA_STATE'], encoding = 'ISO-8859-1')\n",
    "pd_df_txn = pd.read_csv(config['LOCAL']['INPUT_DATA_TXN'], encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>450056063</td>\n",
       "      <td>B.W.E CUSTOM CONSTRUCTION LLC</td>\n",
       "      <td>HC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>450056064</td>\n",
       "      <td>SERGEY NIZHEGORODTSEV PUBLISHING LLC</td>\n",
       "      <td>CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>450056066</td>\n",
       "      <td>SUNRISE ANDOVER LLC</td>\n",
       "      <td>RE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>450056067</td>\n",
       "      <td>474 CENTRAL BOULEVARD LLC</td>\n",
       "      <td>FS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>450056068</td>\n",
       "      <td>ELITE FINISHES LLC</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id                         customer_name sector\n",
       "0    450056063         B.W.E CUSTOM CONSTRUCTION LLC     HC\n",
       "1    450056064  SERGEY NIZHEGORODTSEV PUBLISHING LLC     CS\n",
       "2    450056066                   SUNRISE ANDOVER LLC     RE\n",
       "3    450056067             474 CENTRAL BOULEVARD LLC     FS\n",
       "4    450056068                    ELITE FINISHES LLC   TECH"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let us look at some samples of the data that I loaded\n",
    "pd_df_customer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contact_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>city</th>\n",
       "      <th>state_code</th>\n",
       "      <th>zip</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>status</th>\n",
       "      <th>opportunity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000339</td>\n",
       "      <td>Lyndy</td>\n",
       "      <td>Chachas</td>\n",
       "      <td>Omaha</td>\n",
       "      <td>NE</td>\n",
       "      <td>68130</td>\n",
       "      <td>USA</td>\n",
       "      <td>41.23</td>\n",
       "      <td>-96.18</td>\n",
       "      <td>450058148</td>\n",
       "      <td>Active</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001423</td>\n",
       "      <td>Watts</td>\n",
       "      <td>Eifenstadt</td>\n",
       "      <td>Weston</td>\n",
       "      <td>FL</td>\n",
       "      <td>33326</td>\n",
       "      <td>USA</td>\n",
       "      <td>26.10</td>\n",
       "      <td>-80.36</td>\n",
       "      <td>450059017</td>\n",
       "      <td>Active</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100001837</td>\n",
       "      <td>Jingfeng</td>\n",
       "      <td>Lopina</td>\n",
       "      <td>Hunt Valley</td>\n",
       "      <td>MD</td>\n",
       "      <td>21030</td>\n",
       "      <td>USA</td>\n",
       "      <td>39.50</td>\n",
       "      <td>-76.67</td>\n",
       "      <td>450059076</td>\n",
       "      <td>Active</td>\n",
       "      <td>125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100002544</td>\n",
       "      <td>Gaynell</td>\n",
       "      <td>Vivrett</td>\n",
       "      <td>Beloit</td>\n",
       "      <td>WI</td>\n",
       "      <td>53511</td>\n",
       "      <td>USA</td>\n",
       "      <td>42.50</td>\n",
       "      <td>-89.04</td>\n",
       "      <td>450057762</td>\n",
       "      <td>Active</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100002551</td>\n",
       "      <td>Peregrino</td>\n",
       "      <td>Valles</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>10036</td>\n",
       "      <td>USA</td>\n",
       "      <td>40.76</td>\n",
       "      <td>-73.98</td>\n",
       "      <td>450055995</td>\n",
       "      <td>Active</td>\n",
       "      <td>150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contact_id first_name   last_name         city state_code    zip country  \\\n",
       "0   100000339      Lyndy     Chachas        Omaha         NE  68130     USA   \n",
       "1   100001423      Watts  Eifenstadt       Weston         FL  33326     USA   \n",
       "2   100001837   Jingfeng      Lopina  Hunt Valley         MD  21030     USA   \n",
       "3   100002544    Gaynell     Vivrett       Beloit         WI  53511     USA   \n",
       "4   100002551  Peregrino      Valles     New York         NY  10036     USA   \n",
       "\n",
       "              latitude            longitude  customer_id  status  opportunity  \n",
       "0                41.23               -96.18    450058148  Active        50000  \n",
       "1                26.10               -80.36    450059017  Active        50000  \n",
       "2                39.50               -76.67    450059076  Active       125000  \n",
       "3                42.50               -89.04    450057762  Active        16000  \n",
       "4                40.76               -73.98    450055995  Active       150000  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df_contact.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_ratio</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>ms_rating</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>tna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>Giant</td>\n",
       "      <td>5</td>\n",
       "      <td>VFIAX</td>\n",
       "      <td>Vanguard 500 Index Admiral</td>\n",
       "      <td>163456368456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>Giant</td>\n",
       "      <td>4</td>\n",
       "      <td>VTSAX</td>\n",
       "      <td>Vanguard Total Stock Mkt Idx Adm</td>\n",
       "      <td>136131758268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.04</td>\n",
       "      <td>Giant</td>\n",
       "      <td>5</td>\n",
       "      <td>VINIX</td>\n",
       "      <td>Vanguard Institutional Index I</td>\n",
       "      <td>110407917518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.16</td>\n",
       "      <td>Giant</td>\n",
       "      <td>4</td>\n",
       "      <td>VTSMX</td>\n",
       "      <td>Vanguard Total Stock Mkt Idx Inv</td>\n",
       "      <td>98869371846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.02</td>\n",
       "      <td>Giant</td>\n",
       "      <td>5</td>\n",
       "      <td>VIIIX</td>\n",
       "      <td>Vanguard Institutional Index Instl Pl</td>\n",
       "      <td>93192353649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             exp_ratio market_cap  ms_rating product_id  \\\n",
       "0                 0.05      Giant          5      VFIAX   \n",
       "1                 0.05      Giant          4      VTSAX   \n",
       "2                 0.04      Giant          5      VINIX   \n",
       "3                 0.16      Giant          4      VTSMX   \n",
       "4                 0.02      Giant          5      VIIIX   \n",
       "\n",
       "                            product_name           tna  \n",
       "0             Vanguard 500 Index Admiral  163456368456  \n",
       "1       Vanguard Total Stock Mkt Idx Adm  136131758268  \n",
       "2         Vanguard Institutional Index I  110407917518  \n",
       "3       Vanguard Total Stock Mkt Idx Inv   98869371846  \n",
       "4  Vanguard Institutional Index Instl Pl   93192353649  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df_product.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FS</td>\n",
       "      <td>Financial Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RE</td>\n",
       "      <td>Real Estate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HC</td>\n",
       "      <td>Healthcare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UT</td>\n",
       "      <td>Utilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CS</td>\n",
       "      <td>Communication Services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code             description\n",
       "0   FS      Financial Services\n",
       "1   RE             Real Estate\n",
       "2   HC              Healthcare\n",
       "3   UT               Utilities\n",
       "4   CS  Communication Services"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df_sec_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_code</th>\n",
       "      <th>state</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>Southern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state_code       state    region\n",
       "0         AL     Alabama  Southern\n",
       "1         AK      Alaska   Pacific\n",
       "2         AZ     Arizona   Pacific\n",
       "3         AR    Arkansas  Southern\n",
       "4         CA  California   Pacific"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txn_id</th>\n",
       "      <th>txn_date</th>\n",
       "      <th>contact_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>redemptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422909780</td>\n",
       "      <td>1/2/2015</td>\n",
       "      <td>992808564</td>\n",
       "      <td>VIVAX</td>\n",
       "      <td>46,892.19</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>422909781</td>\n",
       "      <td>1/2/2015</td>\n",
       "      <td>261785827</td>\n",
       "      <td>SOPAX</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-33,424.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>422909782</td>\n",
       "      <td>1/2/2015</td>\n",
       "      <td>389127962</td>\n",
       "      <td>BAICX</td>\n",
       "      <td>14,230.85</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>422909783</td>\n",
       "      <td>1/2/2015</td>\n",
       "      <td>101692476</td>\n",
       "      <td>SGROX</td>\n",
       "      <td>94,046.93</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>422909784</td>\n",
       "      <td>1/2/2015</td>\n",
       "      <td>327754553</td>\n",
       "      <td>FXSIX</td>\n",
       "      <td>22,038.86</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      txn_id  txn_date  contact_id product_id                sales  \\\n",
       "0  422909780  1/2/2015   992808564      VIVAX            46,892.19   \n",
       "1  422909781  1/2/2015   261785827      SOPAX                 0.00   \n",
       "2  422909782  1/2/2015   389127962      BAICX            14,230.85   \n",
       "3  422909783  1/2/2015   101692476      SGROX            94,046.93   \n",
       "4  422909784  1/2/2015   327754553      FXSIX            22,038.86   \n",
       "\n",
       "           redemptions  \n",
       "0                 0.00  \n",
       "1           -33,424.78  \n",
       "2                 0.00  \n",
       "3                 0.00  \n",
       "4                 0.00  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df_txn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Step 2.1: Explore the Data \n",
    "Most of the data from the sources is clean as it comes from application systems which have curated the data well. However, there are a few issues with the data:\n",
    "\n",
    "- Transaction Data: \n",
    "    - This has two columns - one for sales of mutual funds and another for redemptions. For any given transaction, only one of these values is non-zero which means that a transaction is either a sale or a redemption. While this makes it easy for sales and redemptions to be summed up independently, this is not a good model because you will not be able to get Net Sales for any given period without summing up each of those columns and aggregating them. Also, everytime we have to infer the transaction type, we will have to look at one column or another.\n",
    "    - Also, the TXN_DATE column is a string representation of date and needs to be converted to a date representation so that time series analysis can be done on this data\n",
    "\n",
    "- Customer Data: Each customer in the customer data belongs to a sector. Unfortunately, the CRM system maintains a sector code (an abbreviation of the sector) against each customer and has given us a mapping of those codes to names. This degree of normalization is not necessary for our analytical purposes as the only place where we are using this map is in customer data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 2.2: Set up the first part of the data pipleline\n",
    "In order to perform the data cleaning listed above, I would first like to load the data as is into Spark Dataframes. This will be the first step in setting up a data pipeline. Although this step truly belongs in Step 4: Run ETL to Model the Data, I feel that doing it at this stage is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "editable": true,
    "toggleable": false,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-2b1344f2": {
       "style": "primary"
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config('spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CUSTOMER data\n",
    "df_customer = spark.read.format('csv').options(header='true', inferSchema='true').load(config['LOCAL']['INPUT_DATA_CUSTOMER'])\n",
    "\n",
    "df_customer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------+\n",
      "|customer_id|       customer_name|sector|\n",
      "+-----------+--------------------+------+\n",
      "|  450056063|B.W.E CUSTOM CONS...|    HC|\n",
      "|  450056064|SERGEY NIZHEGOROD...|    CS|\n",
      "|  450056066| SUNRISE ANDOVER LLC|    RE|\n",
      "|  450056067|474 CENTRAL BOULE...|    FS|\n",
      "|  450056068|  ELITE FINISHES LLC|  TECH|\n",
      "+-----------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample rows from CUSTOMER data\n",
    "df_customer.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- opportunity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CONTACT data\n",
    "df_contact = spark.read.format('csv').options(header='true', inferSchema='true').load(config['LOCAL']['INPUT_DATA_CONTACT'])\n",
    "\n",
    "df_contact.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+----------+-----+-------+--------+---------+-----------+------+-----------+\n",
      "|contact_id|first_name| last_name|       city|state_code|  zip|country|latitude|longitude|customer_id|status|opportunity|\n",
      "+----------+----------+----------+-----------+----------+-----+-------+--------+---------+-----------+------+-----------+\n",
      "| 100000339|     Lyndy|   Chachas|      Omaha|        NE|68130|    USA|41.22962| -96.1815|  450058148|Active|      50000|\n",
      "| 100001423|     Watts|Eifenstadt|     Weston|        FL|33326|    USA|26.09966|-80.36497|  450059017|Active|      50000|\n",
      "| 100001837|  Jingfeng|    Lopina|Hunt Valley|        MD|21030|    USA|39.50004|-76.66566|  450059076|Active|     125000|\n",
      "| 100002544|   Gaynell|   Vivrett|     Beloit|        WI|53511|    USA|42.49625|-89.03702|  450057762|Active|      16000|\n",
      "| 100002551| Peregrino|    Valles|   New York|        NY|10036|    USA|40.75841|-73.98155|  450055995|Active|     150000|\n",
      "+----------+----------+----------+-----------+----------+-----+-------+--------+---------+-----------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample rows from CONTACT data\n",
    "df_contact.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- exp_ratio: double (nullable = true)\n",
      " |-- market_cap: string (nullable = true)\n",
      " |-- ms_rating: long (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- tna: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read PRODUCT data (note that this data is in JSON format)\n",
    "df_product = spark.read.format('json').options(header='true', inferSchema='true', multiline='true').load(config['LOCAL']['INPUT_DATA_PRODUCT'])\n",
    "\n",
    "df_product.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+----------+--------------------+------------+\n",
      "|exp_ratio|market_cap|ms_rating|product_id|        product_name|         tna|\n",
      "+---------+----------+---------+----------+--------------------+------------+\n",
      "|     0.05|     Giant|        5|     VFIAX|Vanguard 500 Inde...|163456368456|\n",
      "|     0.05|     Giant|        4|     VTSAX|Vanguard Total St...|136131758268|\n",
      "|     0.04|     Giant|        5|     VINIX|Vanguard Institut...|110407917518|\n",
      "|     0.16|     Giant|        4|     VTSMX|Vanguard Total St...| 98869371846|\n",
      "|     0.02|     Giant|        5|     VIIIX|Vanguard Institut...| 93192353649|\n",
      "+---------+----------+---------+----------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample rows from PRODUCT data\n",
    "df_product.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read SECTOR CODES data\n",
    "df_sec_codes = spark.read.format('csv').options(header='true', inferSchema='true').load(config['LOCAL']['INPUT_DATA_SECTOR'])\n",
    "\n",
    "df_sec_codes.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|         description|\n",
      "+----+--------------------+\n",
      "|  FS|  Financial Services|\n",
      "|  RE|         Real Estate|\n",
      "|  HC|          Healthcare|\n",
      "|  UT|           Utilities|\n",
      "|  CS|Communication Ser...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample rows from SECTOR CODE data\n",
    "df_sec_codes.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read STATE data\n",
    "df_state = spark.read.format('csv').options(header='true', inferSchema='true').load(config['LOCAL']['INPUT_DATA_STATE'])\n",
    "\n",
    "df_state.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+\n",
      "|state_code|     state|  region|\n",
      "+----------+----------+--------+\n",
      "|        AL|   Alabama|Southern|\n",
      "|        AK|    Alaska| Pacific|\n",
      "|        AZ|   Arizona| Pacific|\n",
      "|        AR|  Arkansas|Southern|\n",
      "|        CA|California| Pacific|\n",
      "+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample rows from STATE data\n",
    "df_state.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- txn_id: integer (nullable = true)\n",
      " |-- txn_date: string (nullable = true)\n",
      " |-- contact_id: integer (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- sales: double (nullable = true)\n",
      " |-- redemptions: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read TRANSACTION data\n",
    "df_txn = spark.read.format('csv').options(header='true', inferSchema='true').load(config['LOCAL']['INPUT_DATA_TXN'])\n",
    "\n",
    "df_txn.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+----------+---------+-----------+\n",
      "|   txn_id|txn_date|contact_id|product_id|    sales|redemptions|\n",
      "+---------+--------+----------+----------+---------+-----------+\n",
      "|422909780|1/2/2015| 992808564|     VIVAX|46892.193|        0.0|\n",
      "|422909781|1/2/2015| 261785827|     SOPAX|      0.0| -33424.776|\n",
      "|422909782|1/2/2015| 389127962|     BAICX|14230.848|        0.0|\n",
      "|422909783|1/2/2015| 101692476|     SGROX|94046.931|        0.0|\n",
      "|422909784|1/2/2015| 327754553|     FXSIX|22038.856|        0.0|\n",
      "+---------+--------+----------+----------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample rows from TRANSACTION data\n",
    "df_txn.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 2.3: Cleaning Steps\n",
    "- Transaction Data: \n",
    "    - I propose to merge the two columns (sales and redemptions) into one and call it \"txn_amount\" and add a new column called \"txn_type\" which will have the value SALE or REDEMPTION. This makes analyses much easier.\n",
    "    - I am also planning to convert the string representation of TXN_DATE into a date representation so that we can run time series analysis on this data\n",
    "\n",
    "- Customer Data: I am planning to replace the sector code in customer data with the sector name as that will be more meaningful for analyses and will avoid the necessity to do an extra join everytime we want to do sector analysis.\n",
    "\n",
    "- Calendar Data: Finally, I would like to break down the components of the transaction date into its constituent elements (such year, month, quarter, etc.) and store it in its own table. This will help us do time series analyses on the transaction data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+----------+----------+----------+----------+\n",
      "|   txn_id|           txn_date|contact_id|product_id|txn_amount|  txn_type|\n",
      "+---------+-------------------+----------+----------+----------+----------+\n",
      "|422909780|2015-01-02 00:00:00| 992808564|     VIVAX| 46892.193|      SALE|\n",
      "|422909781|2015-01-02 00:00:00| 261785827|     SOPAX|-33424.776|REDEMPTION|\n",
      "|422909782|2015-01-02 00:00:00| 389127962|     BAICX| 14230.848|      SALE|\n",
      "|422909783|2015-01-02 00:00:00| 101692476|     SGROX| 94046.931|      SALE|\n",
      "|422909784|2015-01-02 00:00:00| 327754553|     FXSIX| 22038.856|      SALE|\n",
      "|422909785|2015-01-02 00:00:00| 589519954|     FKUTX|   3853.98|      SALE|\n",
      "|422909786|2015-01-02 00:00:00| 120792413|     AMECX|-53742.108|REDEMPTION|\n",
      "|422909787|2015-01-02 00:00:00| 648471340|     NICSX|-33163.295|REDEMPTION|\n",
      "|422909788|2015-01-02 00:00:00| 483200396|     REREX| 39700.248|      SALE|\n",
      "|422909789|2015-01-02 00:00:00| 719279386|     SHRAX|  30871.19|      SALE|\n",
      "|422909790|2015-01-02 00:00:00| 599365310|     FSICX|-49941.905|REDEMPTION|\n",
      "|422909791|2015-01-02 00:00:00| 993823815|     VWEHX| -6192.448|REDEMPTION|\n",
      "|422909792|2015-01-02 00:00:00| 379647859|     OAKIX| 19416.163|      SALE|\n",
      "|422909793|2015-01-02 00:00:00| 557050782|     ICPAX|-47096.608|REDEMPTION|\n",
      "|422909794|2015-01-02 00:00:00| 157989268|     FHAIX|  42954.57|      SALE|\n",
      "|422909795|2015-01-02 00:00:00| 839604026|     TRRMX|  34528.59|      SALE|\n",
      "|422909796|2015-01-02 00:00:00| 496156226|     FPACX| 50938.056|      SALE|\n",
      "|422909797|2015-01-02 00:00:00| 112672798|     HAINX| 19551.125|      SALE|\n",
      "|422909798|2015-01-02 00:00:00| 306688926|     PRASX| 62443.448|      SALE|\n",
      "|422909799|2015-01-02 00:00:00| 108782571|     FSDAX|-87023.314|REDEMPTION|\n",
      "+---------+-------------------+----------+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean up Transaction table by combining sales and redemptions into a new column called txn_amount, adding a txn_type and dropping the original sales and redemptions columns\n",
    "# We will begin by defining a UDF to determine txn_type\n",
    "def get_txn_type(value):\n",
    "  if   value < 0: \n",
    "      return 'REDEMPTION'\n",
    "  else:\n",
    "      return 'SALE'\n",
    "    \n",
    "# Convert to a UDF Function by passing in the function and return type of function\n",
    "udf_get_txn_type = udf(get_txn_type, StringType())\n",
    "\n",
    "# Add txn_amount column by adding up values in sales and redemptions columns\n",
    "# This is ok because only one of the columns has a value for any given transaction\n",
    "df_txn = df_txn.withColumn('txn_amount', col('sales') + col('redemptions'))\n",
    "\n",
    "# Add txn_type column using the UDF to determine based on the txn_amount\n",
    "df_txn = df_txn.withColumn('txn_type', udf_get_txn_type(col('txn_amount')))\n",
    "\n",
    "# Convert the string representation of TXN_DATE to a date representation\n",
    "df_txn = df_txn.withColumn('txn_date', from_unixtime(unix_timestamp(col('txn_date'), 'MM/dd/yyyy')))\n",
    "\n",
    "# Finally, let us drop the sales and redemptions columns\n",
    "columns_to_drop = ['sales', 'redemptions']\n",
    "df_txn = df_txn.drop(*columns_to_drop)\n",
    "\n",
    "df_txn.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|customer_id|       customer_name|              sector|\n",
      "+-----------+--------------------+--------------------+\n",
      "|  450056095|PASSION FOR CARE LLC|Communication Ser...|\n",
      "|  450056875|MUSSZETT THE POET...|  Financial Services|\n",
      "|  450056528|SHOP CITY DREAMS LLC|         Real Estate|\n",
      "|  450056203|DUNCAN'S CONSTRUC...|Communication Ser...|\n",
      "|  450059400|RICH RHEE INTERAC...|          Healthcare|\n",
      "|  450057029|273 NORTH 1ST STR...|          Technology|\n",
      "|  450057381|ARTYS WINDOW TREA...|         Real Estate|\n",
      "|  450057827|      OMBRE PRET LLC|  Financial Services|\n",
      "|  450058574|ALFA DIGESTIVE DI...|Communication Ser...|\n",
      "|  450059023|322 RAMAPO VALLEY...|          Healthcare|\n",
      "|  450058304|M.I.T.O.S CONSULT...|         Real Estate|\n",
      "|  450055988|INNERSOULDEVELOPM...|           Utilities|\n",
      "|  450056384|     VETS PA 360 LLC|          Healthcare|\n",
      "|  600429450|MONMOUTH TRUCK - ...|          Healthcare|\n",
      "|  450056015|J&T FORMULA SERVI...|  Financial Services|\n",
      "|  450057167|OAK PROPERTY MANA...|         Industrials|\n",
      "|  600429492|WARREN-MOUNTAIN R...|         Real Estate|\n",
      "|  450056448|PINE LAKE LANDING...|          Technology|\n",
      "|  450057968|PJR  AUTOMOTIVE &...|          Healthcare|\n",
      "|  450056605|       STR & SL CORP|         Real Estate|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace the sector code in customer data with the sector name from df_sec_codes by joining them\n",
    "df_customer = df_sec_codes.join(df_customer, df_customer.sector == df_sec_codes.code, 'inner') \\\n",
    "    .distinct() \\\n",
    "    .select(col('customer_id'), col('customer_name'), col('description')) \\\n",
    "    .selectExpr('customer_id as customer_id', 'customer_name as customer_name', 'description as sector')\n",
    "df_customer.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+-----+----+----+-------+\n",
      "|      calendar_date|day|month|year|week|weekday|\n",
      "+-------------------+---+-----+----+----+-------+\n",
      "|2015-04-20 00:00:00| 20|    4|2015|  17|      2|\n",
      "|2015-05-26 00:00:00| 26|    5|2015|  22|      3|\n",
      "|2015-06-23 00:00:00| 23|    6|2015|  26|      3|\n",
      "|2016-04-29 00:00:00| 29|    4|2016|  17|      6|\n",
      "|2017-05-08 00:00:00|  8|    5|2017|  19|      2|\n",
      "|2019-05-24 00:00:00| 24|    5|2019|  21|      6|\n",
      "|2020-01-24 00:00:00| 24|    1|2020|   4|      6|\n",
      "|2020-05-28 00:00:00| 28|    5|2020|  22|      5|\n",
      "|2016-05-04 00:00:00|  4|    5|2016|  18|      4|\n",
      "|2016-10-05 00:00:00|  5|   10|2016|  40|      4|\n",
      "|2015-03-31 00:00:00| 31|    3|2015|  14|      3|\n",
      "|2017-02-22 00:00:00| 22|    2|2017|   8|      4|\n",
      "|2017-04-18 00:00:00| 18|    4|2017|  16|      3|\n",
      "|2017-04-21 00:00:00| 21|    4|2017|  16|      6|\n",
      "|2017-05-18 00:00:00| 18|    5|2017|  20|      5|\n",
      "|2017-10-27 00:00:00| 27|   10|2017|  43|      6|\n",
      "|2018-05-16 00:00:00| 16|    5|2018|  20|      4|\n",
      "|2018-09-14 00:00:00| 14|    9|2018|  37|      6|\n",
      "|2018-10-01 00:00:00|  1|   10|2018|  40|      2|\n",
      "|2019-03-20 00:00:00| 20|    3|2019|  12|      4|\n",
      "+-------------------+---+-----+----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract date components such as day, month, year into its own dataframe\n",
    "# Let us begin with getting a distinct list of dates from df_txn\n",
    "df_calendar = df_txn \\\n",
    "    .distinct() \\\n",
    "    .select(col('txn_date')) \\\n",
    "    .selectExpr('txn_date as calendar_date')\n",
    "\n",
    "# Let us drop the duplicate dates from the CALENDAR\n",
    "df_calendar = df_calendar.drop_duplicates()\n",
    "\n",
    "# Extract date components into separate columns\n",
    "df_calendar = df_calendar.withColumn('day', dayofmonth('calendar_date'))\n",
    "df_calendar = df_calendar.withColumn('month', month('calendar_date'))\n",
    "df_calendar = df_calendar.withColumn('year', year('calendar_date'))\n",
    "df_calendar = df_calendar.withColumn('week', weekofyear('calendar_date'))\n",
    "df_calendar = df_calendar.withColumn('weekday', dayofweek('calendar_date'))\n",
    "\n",
    "df_calendar.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "My conceptual data model will be a star schema with a single fact (transactions) and multiple dimensions that are referenced by that fact. The entities in this conceptual model are as follows:\n",
    "\n",
    "- Transaction (Fact)\n",
    "    - Contact (Dimension) linked to Transaction by CONTACT_ID\n",
    "    \n",
    "    - Customer (Dimension) linked to Contact by CUSTOMER_ID and indirectly linked to Transaction through the Contact\n",
    "    \n",
    "    - Product (Dimension) linked to Transaction by PRODUCT_ID\n",
    "    \n",
    "    - State (Dimension) linked to Contact by STATE_CODE  and indirectly linked to Transaction through Contact\n",
    "    \n",
    "    - Time (Dimension) linked to Transaction by TXN_DATE=>CALENDAR_DATE\n",
    "\n",
    "The conceptual model is captured in an ER Diagram here: https://r766466c839826xjupyterlnnfq3jud.udacity-student-workspaces.com/lab/tree/images/DB_ERD_Capstone_Project_InvestSure.png\n",
    "\n",
    "The data elements of each of these entities is listed below:\n",
    "\n",
    "- Transaction (fact)\n",
    "    - txn_id\n",
    "    - txn_date\n",
    "    - contact_id\n",
    "    - product_id\n",
    "    - txn_amount\n",
    "    - txn_type\n",
    "    \n",
    "- Customer (dimension)\n",
    "    - customer_id\n",
    "    - customer_name\n",
    "    - sector\n",
    "    \n",
    "- Contact (dimension)\n",
    "    - contact_id\n",
    "    - first_name\n",
    "    - last_name\n",
    "    - city\n",
    "    - state_code\n",
    "    - zip\n",
    "    - country\n",
    "    - latitude\n",
    "    - longitude\n",
    "    - customer_id\n",
    "    - status\n",
    "    - opportunity\n",
    "    \n",
    "- State (dimension)\n",
    "    - state_code\n",
    "    - state\n",
    "    - region\n",
    "    \n",
    "- Product (dimension)\n",
    "    - product_id\n",
    "    - product_name\n",
    "    - exp_ratio\n",
    "    - market_cap\n",
    "    - ms_rating\n",
    "    - tna\n",
    "\n",
    "- Caledar (dimension)\n",
    "    - calendar_date\n",
    "    - day\n",
    "    - month\n",
    "    - year\n",
    "    - week\n",
    "    - weekday\n",
    "    \n",
    "All the data will be stored in the destination in *parquet* format. Additionally, Transaction data will also be partitioned by Year and Month of the Transaction Date.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The code snippets above have already loaded all the source data into Spark Dataframes. Additionally, the data cleaning steps above have combined and transformed this data as per suggestions in the Cleaning Steps into structures that could be persisted into tables and stored in the destination as per our Conceptual Model discussed in this section.\n",
    "\n",
    "The flow of data pipeline is as shown below:\n",
    "1. Load source data without modifications into Spark dataframes (completed in Step 2.2)\n",
    "2. Clean data in these dataframes (completed as per suggestions in Step 2.3)\n",
    "3. Load data from the dataframes into *parquet* tables doing the necessary transformations as defined in Step 2.3\n",
    "\n",
    "We will be doing Step 3.2.3 in the cells under Step 4.1 - one section for each target table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create CUSTOMER table\n",
    "customer_columns = ['customer_id', 'customer_name', 'sector']\n",
    "\n",
    "# Write CUSTOMER table to parquet file\n",
    "customer_table = df_customer.select(customer_columns)\n",
    "customer_table.write.mode('overwrite').parquet(config['LOCAL']['OUTPUT_DATA_CUSTOMER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create CONTACT table\n",
    "contact_columns = ['contact_id', 'first_name', 'last_name', 'city', 'state_code', 'zip', 'country', 'latitude', 'longitude', 'customer_id', 'status', 'opportunity']\n",
    "\n",
    "# Write CONTACT table to parquet file\n",
    "contact_table = df_contact.select(contact_columns)\n",
    "contact_table.write.mode('overwrite').parquet(config['LOCAL']['OUTPUT_DATA_CONTACT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create STATE table\n",
    "state_columns = ['state_code', 'state', 'region']\n",
    "\n",
    "# Write STATE table to parquet file\n",
    "state_table = df_state.select(state_columns)\n",
    "state_table.write.mode('overwrite').parquet(config['LOCAL']['OUTPUT_DATA_STATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create PRODUCT table\n",
    "product_columns = ['product_id', 'product_name', 'exp_ratio', 'market_cap', 'ms_rating', 'tna']\n",
    "\n",
    "# Write PRODUCT table to parquet file\n",
    "product_table = df_product.select(product_columns)\n",
    "product_table.write.mode('overwrite').parquet(config['LOCAL']['OUTPUT_DATA_PRODUCT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create CALENDAR table\n",
    "calendar_columns = ['calendar_date', 'day', 'month', 'year', 'week', 'weekday']\n",
    "\n",
    "# Write CALENDAR table to parquet file\n",
    "calendar_table = df_calendar.select(calendar_columns)\n",
    "calendar_table.write.mode('overwrite').partitionBy('year', 'month').parquet(config['LOCAL']['OUTPUT_DATA_CALENDAR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create txn table\n",
    "txn_columns = ['txn_id', 'txn_date', 'contact_id', 'product_id', 'txn_amount', 'txn_type']\n",
    "\n",
    "# Write TXN table to parquet file\n",
    "txn_table = df_txn.join(df_calendar, df_calendar.calendar_date == df_txn.txn_date, 'inner') \\\n",
    "    .select(col('txn_id'), col('txn_date'), col('contact_id'), col('product_id'), col('txn_amount'), col('txn_type'), col('year'), col('month'))\n",
    "txn_table.write.mode('overwrite').partitionBy('year', 'month').parquet(config['LOCAL']['OUTPUT_DATA_TXN'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "I would like to implement the following data quality checks:\n",
    "- Completeness Checks\n",
    "    - Match row counts between source tables and analytical tables (after accounting for duplicate removal)\n",
    "    - Confirm that CALENDAR and TXN tables have been partitioned by Year and Month (visual inspection)\n",
    "    \n",
    "- Data Integrity Checks\n",
    "    - All foreign key values in analytical tables are matched by corresponding primary key values in the parent tables\n",
    " \n",
    "These quality checks will be run by the code in the cells in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts for CUSTOMER table matched between source and analytical tables\n"
     ]
    }
   ],
   "source": [
    "# Completeness Checks - matching row counts\n",
    "\n",
    "# CUSTOMER table\n",
    "assert len(pd_df_customer.index) == spark.read.parquet(config['LOCAL']['OUTPUT_DATA_CUSTOMER']).count()\n",
    "print('Row counts for CUSTOMER table matched between source and analytical tables')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts for CONTACT table matched between source and analytical tables\n"
     ]
    }
   ],
   "source": [
    "# CONTACT table\n",
    "assert len(pd_df_contact.index) == spark.read.parquet(config['LOCAL']['OUTPUT_DATA_CONTACT']).count()\n",
    "print('Row counts for CONTACT table matched between source and analytical tables')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts for PRODUCT table matched between source and analytical tables\n"
     ]
    }
   ],
   "source": [
    "# PRODUCT table\n",
    "assert len(pd_df_product.index) == spark.read.parquet(config['LOCAL']['OUTPUT_DATA_PRODUCT']).count()\n",
    "print('Row counts for PRODUCT table matched between source and analytical tables')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts for STATE table matched between source and analytical tables\n"
     ]
    }
   ],
   "source": [
    "# STATE table\n",
    "assert len(pd_df_state.index) == spark.read.parquet(config['LOCAL']['OUTPUT_DATA_STATE']).count()\n",
    "print('Row counts for STATE table matched between source and analytical tables')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts for STATE table matched between source and analytical tables\n"
     ]
    }
   ],
   "source": [
    "# CALENDAR table\n",
    "assert len(pd.unique(pd_df_txn['txn_date'])) == spark.read.parquet(config['LOCAL']['OUTPUT_DATA_CALENDAR']).count()\n",
    "print('Row counts for CALENDAR table matched between source and analytical tables')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts for TXN table matched between source and analytical tables\n"
     ]
    }
   ],
   "source": [
    "# TXN table\n",
    "assert len(pd_df_txn.index) == spark.read.parquet(config['LOCAL']['OUTPUT_DATA_TXN']).count()\n",
    "print('Row counts for TXN table matched between source and analytical tables')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
